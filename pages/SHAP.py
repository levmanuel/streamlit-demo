import streamlit as st
import pandas as pd
import shap
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split

st.set_page_config(page_title="SHAP & IA - Explicabilit√©", layout="wide")

st.title("üß† Explicabilit√© locale et globale avec SHAP")
st.write("""
Cette application explore les concepts d‚Äô**explicabilit√© globale et locale** √† l‚Äôaide de **SHAP**.
Nous utilisons un mod√®le de **Random Forest** sur le dataset *California Housing* pour pr√©dire le prix moyen d‚Äôune maison.
""")

st.header("üì¶ 1. Chargement et pr√©paration des donn√©es")
@st.cache_data
def load_data():
    data = fetch_california_housing(as_frame=True)
    return data.data, data.target

X, y = load_data()
st.write("Aper√ßu des donn√©es :")
st.dataframe(X.head())

st.header("üß† 2. Mod√®le utilis√©")
st.markdown("""
Nous utilisons un mod√®le **Random Forest Regressor** de Scikit-learn :
- `n_estimators = 100`
- `random_state = 42`
""")

model = RandomForestRegressor(n_estimators=100, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model.fit(X_train, y_train)
st.success("‚úÖ Mod√®le entra√Æn√© avec succ√®s !")

st.header("üìä 3. Performance du mod√®le")
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

col1, col2 = st.columns(2)
col1.metric("üìà R¬≤ score", f"{r2:.3f}")
col2.metric("üìâ Erreur absolue moyenne", f"{mae:.3f}")

st.subheader("Comparaison Pr√©dictions vs R√©el (√©chantillon)")
fig_perf, ax_perf = plt.subplots()
ax_perf.scatter(y_test[:100], y_pred[:100], alpha=0.6)
ax_perf.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
ax_perf.set_xlabel("Vrai prix")
ax_perf.set_ylabel("Prix pr√©dit")
ax_perf.set_title("Pr√©dictions vs Valeurs r√©elles")
st.pyplot(fig_perf)

# ------------------------------------
# Am√©liorations pour la section d'explicabilit√© globale
st.header("üåê 4. Explicabilit√© Globale")
st.write("Analyse de l'influence moyenne des variables √† l'aide de SHAP.")

st.markdown("""
#### üìò Comment lire ce graphique ?

Ce graphique montre l'**importance moyenne** de chaque variable dans les pr√©dictions du mod√®le, mais d'une mani√®re plus riche que les importances classiques de Random Forest.

- **Chaque barre** repr√©sente une variable du mod√®le
- **Plus la barre est longue**, plus cette variable a d'impact sur les pr√©dictions en moyenne
- L'importance est mesur√©e par la **valeur absolue moyenne des SHAP values** pour chaque variable
- Contrairement aux mesures d'importance classiques, SHAP:
  - Est coh√©rent math√©matiquement (bas√© sur la th√©orie des jeux)
  - Tient compte des interactions entre variables
  - Consid√®re l'impact r√©el sur chaque pr√©diction individuelle

**Interpr√©tation pour ce mod√®le:**
- `MedInc` (revenu m√©dian) est le facteur le plus d√©terminant pour pr√©dire le prix des logements
- `Latitude`, `AveOccup` (occupation moyenne) et `Longitude` jouent √©galement un r√¥le important
- Les variables avec des barres plus courtes comme `HouseAge` ont un impact global plus limit√©
""")

@st.cache_resource
def get_explainer():
    return shap.TreeExplainer(model)

explainer = get_explainer()

# R√©duire la taille pour √©viter lenteurs
X_sample = X_test.iloc[:min(100, len(X_test))]
shap_values = explainer.shap_values(X_sample)

# Graphique d'importance
st.subheader("üîç Graphique des importances moyennes (SHAP)")
fig_global, ax_global = plt.subplots(figsize=(10, 6))
shap.summary_plot(shap_values, X_sample, plot_type="bar", show=False)
plt.tight_layout()
st.pyplot(fig_global)

# Ajout d'un graphique de d√©pendance pour la variable la plus importante
st.subheader("üìà Graphique de d√©pendance pour le revenu m√©dian")
st.markdown("""
Ce graphique montre comment la variable `MedInc` (revenu m√©dian) influence les pr√©dictions:
- L'axe X repr√©sente les valeurs de revenu m√©dian
- L'axe Y repr√©sente l'impact SHAP (contribution √† la pr√©diction)
- Chaque point est une observation
- Les couleurs indiquent une autre variable qui interagit potentiellement avec MedInc

On observe une **relation positive non-lin√©aire**: quand le revenu m√©dian augmente, l'impact sur le prix pr√©dit augmente aussi, mais pas de fa√ßon parfaitement lin√©aire.
""")

fig_depend, ax_depend = plt.subplots(figsize=(10, 6))
shap.dependence_plot("MedInc", shap_values, X_sample, ax=ax_depend, show=False)
plt.tight_layout()
st.pyplot(fig_depend)

# Am√©liorations pour la section d'explicabilit√© locale
st.header("üîé 5. Explicabilit√© Locale")
index = st.slider("S√©lectionnez une observation √† expliquer :", 0, len(X_sample) - 1, 0)
individual = X_sample.iloc[[index]]

pred_value = model.predict(individual)[0]
base_value = explainer.expected_value

st.write("Observation s√©lectionn√©e :")
st.write(individual)
# Afficher la pr√©diction de fa√ßon plus s√ªre
st.write(f"**Prix pr√©dit:** {pred_value:.3f}")
st.write(f"**Valeur de base (moyenne du mod√®le):** {float(base_value):.3f}")
st.write(f"**Diff√©rence:** {pred_value - float(base_value):.3f}")

st.markdown("""
#### üìò Comment lire ce graphique ?

Le waterfall plot explique la **pr√©diction pour une observation individuelle** en d√©taillant la contribution de chaque variable.

- **La base value (valeur de base)** est la moyenne des pr√©dictions sur tout le dataset - c'est ce que pr√©dirait le mod√®le sans aucune information sur cette observation sp√©cifique
- **Chaque ligne** montre comment une variable particuli√®re pousse la pr√©diction:
  - **En rouge**: la variable augmente la pr√©diction par rapport √† la moyenne
  - **En bleu**: la variable diminue la pr√©diction par rapport √† la moyenne
- **La taille** de chaque barre correspond √† l'ampleur de l'impact
- **`f(x)`** est la pr√©diction finale pour cette observation, r√©sultat de toutes ces contributions combin√©es

**Comment utiliser cette explication:**
- Pour les professionnels immobiliers: comprendre les facteurs qui valorisent ou d√©valorisent un bien sp√©cifique
- Pour les data scientists: d√©tecter des anomalies ou biais potentiels dans le mod√®le
- Pour les d√©cideurs: expliquer de mani√®re transparente pourquoi une pr√©diction particuli√®re a √©t√© faite
""")

st.subheader("üìà Waterfall plot de la pr√©diction")
fig_local, ax_local = plt.subplots(figsize=(10, 8))
shap.plots.waterfall(shap.Explanation(
    values=shap_values[index],
    base_values=explainer.expected_value,
    data=individual.values[0],
    feature_names=individual.columns.tolist()), show=False)
plt.tight_layout()
st.pyplot(fig_local)

# Nouvelle section: comparaison explicabilit√© globale vs locale
st.header("üîÑ 6. Comparaison Explicabilit√© Globale vs Locale")
st.markdown("""
### Quelle approche choisir?

**Explicabilit√© Globale** | **Explicabilit√© Locale**
--------------------------|-------------------------
Vue d'ensemble du mod√®le | Analyse d'une pr√©diction sp√©cifique
Montre les tendances g√©n√©rales | Explique un cas particulier
Utile pour comprendre le mod√®le | Utile pour justifier une d√©cision
Ne capture pas les comportements complexes | Ne r√©v√®le pas n√©cessairement les tendances g√©n√©rales

### Cas d'usage:

- **D√©veloppement de mod√®le**: utilisez l'explicabilit√© globale pour valider que votre mod√®le se base sur des variables pertinentes
- **Audit et conformit√©**: utilisez l'explicabilit√© locale pour justifier des d√©cisions individuelles
- **Communication**: combinaison des deux approches pour une compr√©hension compl√®te

### En pratique:
- Commencez par l'explicabilit√© globale pour comprendre le comportement g√©n√©ral du mod√®le
- Utilisez l'explicabilit√© locale pour investiguer des cas particuliers ou des anomalies
- Pr√©sentez toujours les limites de ces interpr√©tations (corr√©lation ‚â† causalit√©)
""")

# Nouvelle section: visualisation avanc√©e des interactions
st.header("üß© 7. Visualisation avanc√©e des interactions")
st.markdown("""
### Visualisation des interactions complexes

La visualisation de type "SHAP Summary Plot" ci-dessous montre la distribution des valeurs SHAP pour chaque variable:
- Chaque point repr√©sente une observation (une maison)
- La position sur l'axe horizontal indique la valeur SHAP (impact sur la pr√©diction)
- Les couleurs indiquent si la valeur de la variable est √©lev√©e (rouge) ou basse (bleu)
- On peut ainsi rep√©rer des motifs et des interactions entre variables

**Comment l'interpr√©ter:**
- Un gradient de couleur (bleu √† rouge) qui va de gauche √† droite indique une relation positive
- Un gradient invers√© (rouge √† gauche, bleu √† droite) indique une relation n√©gative
- Des motifs non-lin√©aires sugg√®rent des interactions complexes ou des effets de seuil
""")

st.subheader("Summary Plot des impacts variables")
fig_summary, ax_summary = plt.subplots(figsize=(10, 8))
shap.summary_plot(shap_values, X_sample, show=False)
plt.tight_layout()
st.pyplot(fig_summary)

# Ajout d'une visualisation pour une observation unique
st.subheader("Force Plot pour l'observation s√©lectionn√©e")
st.markdown("""
Ce graphique montre comment chaque variable contribue √† la pr√©diction pour l'observation s√©lectionn√©e:
- Les variables rouges poussent la pr√©diction vers le haut
- Les variables bleues poussent la pr√©diction vers le bas
- La largeur de chaque segment correspond √† l'ampleur de l'impact
""")
fig_force_single, ax_force_single = plt.subplots(figsize=(10, 3))
shap.force_plot(explainer.expected_value, shap_values[index], individual.values[0], 
               feature_names=individual.columns.tolist(), matplotlib=True, show=False)
plt.tight_layout()
st.pyplot(fig_force_single)