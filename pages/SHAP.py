# Am√©liorations pour la section d'explicabilit√© globale
st.header("üåê 4. Explicabilit√© Globale")
st.write("Analyse de l'influence moyenne des variables √† l'aide de SHAP.")

st.markdown("""
#### üìò Comment lire ce graphique ?

Ce graphique montre l'**importance moyenne** de chaque variable dans les pr√©dictions du mod√®le, mais d'une mani√®re plus riche que les importances classiques de Random Forest.

- **Chaque barre** repr√©sente une variable du mod√®le
- **Plus la barre est longue**, plus cette variable a d'impact sur les pr√©dictions en moyenne
- L'importance est mesur√©e par la **valeur absolue moyenne des SHAP values** pour chaque variable
- Contrairement aux mesures d'importance classiques, SHAP:
  - Est coh√©rent math√©matiquement (bas√© sur la th√©orie des jeux)
  - Tient compte des interactions entre variables
  - Consid√®re l'impact r√©el sur chaque pr√©diction individuelle

**Interpr√©tation pour ce mod√®le:**
- `MedInc` (revenu m√©dian) est le facteur le plus d√©terminant pour pr√©dire le prix des logements
- `Latitude`, `AveOccup` (occupation moyenne) et `Longitude` jouent √©galement un r√¥le important
- Les variables avec des barres plus courtes comme `HouseAge` ont un impact global plus limit√©
""")

@st.cache_resource
def get_explainer():
    return shap.TreeExplainer(model)

explainer = get_explainer()

# R√©duire la taille pour √©viter lenteurs
X_sample = X_test.iloc[:min(100, len(X_test))]
shap_values = explainer.shap_values(X_sample)

# Graphique d'importance
st.subheader("üîç Graphique des importances moyennes (SHAP)")
fig_global, ax_global = plt.subplots(figsize=(10, 6))
shap.summary_plot(shap_values, X_sample, plot_type="bar", show=False)
plt.tight_layout()
st.pyplot(fig_global)

# Ajout d'un graphique de d√©pendance pour la variable la plus importante
st.subheader("üìà Graphique de d√©pendance pour le revenu m√©dian")
st.markdown("""
Ce graphique montre comment la variable `MedInc` (revenu m√©dian) influence les pr√©dictions:
- L'axe X repr√©sente les valeurs de revenu m√©dian
- L'axe Y repr√©sente l'impact SHAP (contribution √† la pr√©diction)
- Chaque point est une observation
- Les couleurs indiquent une autre variable qui interagit potentiellement avec MedInc

On observe une **relation positive non-lin√©aire**: quand le revenu m√©dian augmente, l'impact sur le prix pr√©dit augmente aussi, mais pas de fa√ßon parfaitement lin√©aire.
""")

fig_depend, ax_depend = plt.subplots(figsize=(10, 6))
shap.dependence_plot("MedInc", shap_values, X_sample, ax=ax_depend, show=False)
plt.tight_layout()
st.pyplot(fig_depend)

# Am√©liorations pour la section d'explicabilit√© locale
st.header("üîé 5. Explicabilit√© Locale")
index = st.slider("S√©lectionnez une observation √† expliquer :", 0, len(X_sample) - 1, 0)
individual = X_sample.iloc[[index]]

pred_value = model.predict(individual)[0]
base_value = explainer.expected_value

st.write("Observation s√©lectionn√©e :")
st.write(individual)
st.metric("Prix pr√©dit", f"{pred_value:.3f}", f"{pred_value - base_value:.3f} par rapport √† la moyenne")

st.markdown("""
#### üìò Comment lire ce graphique ?

Le waterfall plot explique la **pr√©diction pour une observation individuelle** en d√©taillant la contribution de chaque variable.

- **La base value (valeur de base)** est la moyenne des pr√©dictions sur tout le dataset - c'est ce que pr√©dirait le mod√®le sans aucune information sur cette observation sp√©cifique
- **Chaque ligne** montre comment une variable particuli√®re pousse la pr√©diction:
  - **En rouge**: la variable augmente la pr√©diction par rapport √† la moyenne
  - **En bleu**: la variable diminue la pr√©diction par rapport √† la moyenne
- **La taille** de chaque barre correspond √† l'ampleur de l'impact
- **`f(x)`** est la pr√©diction finale pour cette observation, r√©sultat de toutes ces contributions combin√©es

**Comment utiliser cette explication:**
- Pour les professionnels immobiliers: comprendre les facteurs qui valorisent ou d√©valorisent un bien sp√©cifique
- Pour les data scientists: d√©tecter des anomalies ou biais potentiels dans le mod√®le
- Pour les d√©cideurs: expliquer de mani√®re transparente pourquoi une pr√©diction particuli√®re a √©t√© faite
""")

st.subheader("üìà Waterfall plot de la pr√©diction")
fig_local, ax_local = plt.subplots(figsize=(10, 8))
shap.plots.waterfall(shap.Explanation(
    values=shap_values[index],
    base_values=explainer.expected_value,
    data=individual.values[0],
    feature_names=individual.columns.tolist()), show=False)
plt.tight_layout()
st.pyplot(fig_local)

# Nouvelle section: comparaison explicabilit√© globale vs locale
st.header("üîÑ 6. Comparaison Explicabilit√© Globale vs Locale")
st.markdown("""
### Quelle approche choisir?

**Explicabilit√© Globale** | **Explicabilit√© Locale**
--------------------------|-------------------------
Vue d'ensemble du mod√®le | Analyse d'une pr√©diction sp√©cifique
Montre les tendances g√©n√©rales | Explique un cas particulier
Utile pour comprendre le mod√®le | Utile pour justifier une d√©cision
Ne capture pas les comportements complexes | Ne r√©v√®le pas n√©cessairement les tendances g√©n√©rales

### Cas d'usage:

- **D√©veloppement de mod√®le**: utilisez l'explicabilit√© globale pour valider que votre mod√®le se base sur des variables pertinentes
- **Audit et conformit√©**: utilisez l'explicabilit√© locale pour justifier des d√©cisions individuelles
- **Communication**: combinaison des deux approches pour une compr√©hension compl√®te

### En pratique:
- Commencez par l'explicabilit√© globale pour comprendre le comportement g√©n√©ral du mod√®le
- Utilisez l'explicabilit√© locale pour investiguer des cas particuliers ou des anomalies
- Pr√©sentez toujours les limites de ces interpr√©tations (corr√©lation ‚â† causalit√©)
""")

# Nouvelle section: force plot pour visualiser les interactions entre variables
st.header("üß© 7. Interactions entre variables (Force Plot)")
st.markdown("""
### Visualisation des interactions complexes

Le force plot ci-dessous montre comment toutes les variables interagissent pour produire chaque pr√©diction:
- Chaque point repr√©sente une observation (une maison)
- La position sur l'axe horizontal indique la valeur SHAP (impact sur la pr√©diction)
- Les couleurs indiquent si la valeur de la variable est √©lev√©e (rouge) ou basse (bleu)
- On peut ainsi rep√©rer des motifs r√©currents et des interactions entre variables

**Comment l'interpr√©ter:**
Cherchez des motifs de couleur qui se r√©p√®tent. Par exemple, si des points avec revenu √©lev√© (rouge) et √¢ge faible (bleu) sont syst√©matiquement √† droite, cela sugg√®re une interaction entre ces variables.
""")

st.subheader("Force Plot pour les 50 premi√®res observations")
fig_force, ax_force = plt.subplots(figsize=(10, 3))
shap_values_subset = shap_values[:50]
X_sample_subset = X_sample.iloc[:50]
shap.force_plot(explainer.expected_value, shap_values_subset, X_sample_subset, matplotlib=True, show=False, plot_cmap=['#FF4B4B', '#4B4BFF'])
plt.tight_layout()
st.pyplot(fig_force)